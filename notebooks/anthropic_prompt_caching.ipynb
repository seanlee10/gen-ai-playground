{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa7/voiq34dkfRSd8LQf1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanlee10/gen-ai-playground/blob/main/notebooks/anthropic_prompt_caching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Caching\n",
        "\n",
        "Prompt caching is a powerful feature that optimizes your API usage by allowing resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements.\n",
        "\n",
        "The cache has a 5 minute time to live (TTL). Currently, “ephemeral” is the only supported cache type, which corresponds to this 5-minute lifetime. Find out more at https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching"
      ],
      "metadata": {
        "id": "XBIn09vrZD51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "zSKLy100RbDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU yt-dlp anthropic"
      ],
      "metadata": {
        "id": "cPXEVlQAUTED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve Content from Notion"
      ],
      "metadata": {
        "id": "jZ-zklEPSwoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CmUpFMcdRVGf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import anthropic\n",
        "from google.colab import userdata\n",
        "\n",
        "auth_token = userdata.get('notion')\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=userdata.get('anthropic')\n",
        ")\n",
        "\n",
        "def fetch_content(page_id):\n",
        "  # URL of the API endpoint\n",
        "  url = f\"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100\"\n",
        "\n",
        "  # Headers including the authorization\n",
        "  headers = {\n",
        "      \"Authorization\": f\"Bearer {auth_token}\",\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Notion-Version\": \"2022-02-22\"\n",
        "  }\n",
        "\n",
        "  try:\n",
        "      # Make the GET request\n",
        "      response = requests.get(url, headers=headers)\n",
        "      content = \"\"\n",
        "\n",
        "      # Check if the request was successful\n",
        "      if response.status_code == 200:\n",
        "          # Request was successful\n",
        "          data = response.json()  # Assuming the response is in JSON format\n",
        "\n",
        "          for block in data['results']:\n",
        "            if block['type'] == 'paragraph':\n",
        "              content += block['paragraph']['rich_text'][0]['text']['content']\n",
        "\n",
        "          print(\"Request successful!\")\n",
        "          return content\n",
        "      else:\n",
        "          # Request failed\n",
        "          print(f\"Request failed with status code: {response.status_code}\")\n",
        "          print(\"Response content:\", response.text)\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      # Handle any exceptions that occurred during the request\n",
        "      print(f\"An error occurred: {e}\")\n",
        "      raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Retrival of a Notion Page"
      ],
      "metadata": {
        "id": "dVBzHj58W2RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the Page ID with your own\n",
        "content = fetch_content(\"17d11dd8536b804f81c0e280dd688f8f\")\n",
        "print(content)"
      ],
      "metadata": {
        "id": "g2EKSC-aSdj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define an function for regular API Call"
      ],
      "metadata": {
        "id": "zdtzYu20W9kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_api_call():\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"<transcript>\" + content + \"</transcript>\",\n",
        "              },\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"Who is the interviewee of this interview? Only output the name\"\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  start_time = time.time()\n",
        "  response = client.messages.create(\n",
        "      model=\"claude-3-5-sonnet-latest\",\n",
        "      max_tokens=300,\n",
        "      messages=messages,\n",
        "  )\n",
        "  end_time = time.time()\n",
        "\n",
        "  return response, end_time - start_time"
      ],
      "metadata": {
        "id": "0RkwJKH7TqpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define an function for Cached API Call"
      ],
      "metadata": {
        "id": "fkrPajowXDXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cached_api_call():\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"<transcript>\" + content + \"</transcript>\",\n",
        "                  \"cache_control\": {\"type\": \"ephemeral\"}\n",
        "              },\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": \"Who is the interviewee of this interview? Only output the name\"\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  start_time = time.time()\n",
        "  response = client.messages.create(\n",
        "      model=\"claude-3-5-sonnet-latest\",\n",
        "      max_tokens=300,\n",
        "      messages=messages,\n",
        "      extra_headers={\"anthropic-beta\":\"prompt-caching-2024-07-31\"}\n",
        "  )\n",
        "  end_time = time.time()\n",
        "\n",
        "  return response, end_time - start_time\n"
      ],
      "metadata": {
        "id": "T37GxxnlVru2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the difference"
      ],
      "metadata": {
        "id": "qdcfcn6_XGTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response, call_time = make_api_call()\n",
        "\n",
        "print(f\"API call time: {call_time:.2f} seconds\")\n",
        "print(f\"API call input tokens: {response.usage.input_tokens}\")\n",
        "print(f\"API call output tokens: {response.usage.output_tokens}\")\n",
        "\n",
        "print(\"\\nResult:\")\n",
        "print(response.content)\n",
        "\n",
        "cached_response, cached_time = make_cached_api_call()\n",
        "\n",
        "print(f\"Cached API call time: {cached_time:.2f} seconds\")\n",
        "print(f\"Cached API call input tokens: {cached_response.usage.input_tokens}\")\n",
        "print(f\"Cached API call output tokens: {cached_response.usage.output_tokens}\")\n",
        "\n",
        "print(\"\\nResult:\")\n",
        "print(cached_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da8QzDAuWrRB",
        "outputId": "6b0fb562-0a02-4258-aecf-6b7f7fecfd18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API call time: 2.43 seconds\n",
            "API call input tokens: 18234\n",
            "API call output tokens: 20\n",
            "\n",
            "Result:\n",
            "[TextBlock(text='The interviewee is Jensen Huang, who is the CEO of NVIDIA.', type='text')]\n",
            "Cached API call time: 1.41 seconds\n",
            "Cached API call input tokens: 18\n",
            "Cached API call output tokens: 17\n",
            "\n",
            "Result:\n",
            "[TextBlock(text='Jensen Huang, the CEO and co-founder of NVIDIA.', type='text')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFiUJXMoWsEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}