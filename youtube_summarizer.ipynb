{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanlee10/gen-ai-playground/blob/main/youtube_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPDLYd1T2nLn",
        "outputId": "aff6b49e-a0cd-429d-b71a-b23e8c37dea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU yt-dlp boto3 deepgram-sdk langchain-aws langchain-core langchain-groq langchain-nvidia-ai-endpoints langchain-anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT8Dl-sO2tB5",
        "outputId": "554681d2-a57a-4d98-c491-9502bf2cef01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the video url: https://www.youtube.com/watch?v=IxkvVZua28k\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=IxkvVZua28k\n",
            "[youtube] IxkvVZua28k: Downloading webpage\n",
            "[youtube] IxkvVZua28k: Downloading ios player API JSON\n",
            "[youtube] IxkvVZua28k: Downloading mweb player API JSON\n",
            "[youtube] IxkvVZua28k: Downloading player baafab19\n",
            "[youtube] IxkvVZua28k: Downloading m3u8 information\n",
            "[info] IxkvVZua28k: Downloading 1 format(s): 251\n",
            "[download] Destination: download.mp3\n",
            "[download] 100% of   40.75MiB in 00:00:03 at 12.39MiB/s  \n",
            "a_conversation_with_open_ai_s_cpo_kevin_weil_anthropic_s_cpo_mike_krieger_and_sarah_guo\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=IxkvVZua28k\n",
            "[youtube] IxkvVZua28k: Downloading webpage\n",
            "[youtube] IxkvVZua28k: Downloading ios player API JSON\n",
            "[youtube] IxkvVZua28k: Downloading mweb player API JSON\n",
            "[youtube] IxkvVZua28k: Downloading m3u8 information\n",
            "[info] IxkvVZua28k: Downloading 1 format(s): 251\n",
            "[download] download.mp3 has already been downloaded\n",
            "[download] 100% of   40.75MiB\n",
            "Successfully Downloaded - see local folder on Google Colab\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from yt_dlp import YoutubeDL\n",
        "\n",
        "video_url = input('Enter the video url: ')\n",
        "\n",
        "def to_snake_case(string):\n",
        "    # Replace any non-word character with an underscore\n",
        "    s1 = re.sub(r'[^\\w]+', '_', string)\n",
        "    # Insert an underscore before any uppercase letter which is preceded by a lowercase letter or number\n",
        "    s2 = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
        "    # Convert to lowercase\n",
        "    return s2.lower()\n",
        "\n",
        "def download_audio(link):\n",
        "  with YoutubeDL({'extract_audio': True, 'format': 'bestaudio', 'outtmpl': 'download.mp3'}) as video:\n",
        "    info_dict = video.extract_info(link, download = True)\n",
        "    video_title = to_snake_case(info_dict['title'])\n",
        "    print(video_title)\n",
        "    video.download(link)\n",
        "    print(\"Successfully Downloaded - see local folder on Google Colab\")\n",
        "\n",
        "    return video_title\n",
        "\n",
        "video_title = download_audio(video_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUeIC5cYi3UQ"
      },
      "outputs": [],
      "source": [
        "from deepgram import (\n",
        "    DeepgramClient,\n",
        "    DeepgramClientOptions,\n",
        "    PrerecordedOptions,\n",
        "    FileSource,\n",
        ")\n",
        "from deepgram.utils import verboselogs\n",
        "import httpx\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "\n",
        "AUDIO_FILE = 'download.mp3'\n",
        "\n",
        "try:\n",
        "    # STEP 1 Create a Deepgram client using the API key in the environment variables\n",
        "    deepgram: DeepgramClient = DeepgramClient(api_key=userdata.get('deepgram'))\n",
        "    # OR use defaults\n",
        "    # deepgram: DeepgramClient = DeepgramClient()\n",
        "\n",
        "    # STEP 2 Call the transcribe_file method on the rest class\n",
        "    with open(AUDIO_FILE, \"rb\") as file:\n",
        "        buffer_data = file.read()\n",
        "\n",
        "    payload: FileSource = {\n",
        "        \"buffer\": buffer_data,\n",
        "    }\n",
        "\n",
        "    options: PrerecordedOptions = PrerecordedOptions(\n",
        "        model=\"nova-2\",\n",
        "        smart_format=True,\n",
        "    )\n",
        "\n",
        "    before = datetime.now()\n",
        "    response = deepgram.listen.rest.v(\"1\").transcribe_file(\n",
        "        payload, options, timeout=httpx.Timeout(300.0, connect=10.0)\n",
        "    )\n",
        "    after = datetime.now()\n",
        "\n",
        "    print(response.to_json(indent=4))\n",
        "    print(\"\")\n",
        "    difference = after - before\n",
        "    print(f\"time: {difference.seconds}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZJikwtimlwG"
      },
      "outputs": [],
      "source": [
        "transcription = response['results']['channels'][0]['alternatives'][0]['transcript']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8lQ-sKmUOzz",
        "outputId": "c0ce425f-4d0d-4f62-82fa-ff81ed7e942a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload Successful: download.mp3 uploaded to cdk-hnb659fds-assets-172054518491-ap-northeast-2/elon_s_predictions_for_the_future_ai_free_energy_and_more_ep_129.mp3\n",
            "File upload completed\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "from google.colab import userdata\n",
        "\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=userdata.get('aws_access_key_id'),\n",
        "    aws_secret_access_key=userdata.get('aws_secret_access_key'),\n",
        "    region_name=\"ap-northeast-2\"\n",
        ")\n",
        "\n",
        "s3 = session.client('s3')\n",
        "transcribe = session.client('transcribe')\n",
        "\n",
        "local_file = 'download.mp3'\n",
        "bucket = 'cdk-hnb659fds-assets-172054518491-ap-northeast-2'\n",
        "s3_file = f\"{video_title}.mp3\"\n",
        "\n",
        "def upload_to_s3(local_file, bucket, s3_file):\n",
        "    s3 = session.client('s3')\n",
        "\n",
        "    try:\n",
        "        s3.upload_file(local_file, bucket, s3_file)\n",
        "        print(f\"Upload Successful: {local_file} uploaded to {bucket}/{s3_file}\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file {local_file} was not found\")\n",
        "        return False\n",
        "\n",
        "uploaded = upload_to_s3(local_file, bucket, s3_file)\n",
        "\n",
        "if uploaded:\n",
        "    print(\"File upload completed\")\n",
        "else:\n",
        "    print(\"File upload failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCsLRPIjUNld",
        "outputId": "22f67074-b91f-4609-ed78-90e29bf71f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription job created: elon_s_predictions_for_the_future_ai_free_energy_and_more_ep_129\n"
          ]
        }
      ],
      "source": [
        "def create_transcribe_job(job_name, audio_file_uri):\n",
        "    response = transcribe.start_transcription_job(\n",
        "        TranscriptionJobName=job_name,\n",
        "        LanguageCode='en-US',  # Specify the language code\n",
        "        MediaFormat='mp3',  # Specify the media format (e.g., mp3, wav)\n",
        "        Media={\n",
        "            'MediaFileUri': audio_file_uri\n",
        "        },\n",
        "        Settings={\n",
        "            'ShowSpeakerLabels': True,\n",
        "            'MaxSpeakerLabels': 3\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "job_name = video_title\n",
        "audio_file_uri = f\"s3://{bucket}/{s3_file}\"\n",
        "\n",
        "response = create_transcribe_job(job_name, audio_file_uri)\n",
        "print(f\"Transcription job created: {response['TranscriptionJob']['TranscriptionJobName']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LMMgZRR6gCsN",
        "outputId": "2c3ebf2e-e96d-420e-f47a-1337d00b8382"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'COMPLETED'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = transcribe.get_transcription_job(\n",
        "    TranscriptionJobName=job_name\n",
        ")\n",
        "\n",
        "response['TranscriptionJob']['TranscriptionJobStatus']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL2Me6YChA2f",
        "outputId": "c4a8dd71-c777-4dd1-c632-2841134e8e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://s3.ap-northeast-2.amazonaws.com/aws-transcribe-ap-northeast-2-prod/172054518491/elon_s_predictions_for_the_future_ai_free_energy_and_more_ep_129/86785581-8796-4bd5-9cae-c441d09b47d3/asrOutput.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDmFwLW5vcnRoZWFzdC0yIkcwRQIgL7mITV5ERR6W1nAr2Hc5TMXnWc0v6FSsaWbFB2IPdm0CIQDqmX%2FRM3YXtPSHhs%2Fy%2F8YZTfOFHRIX2qSRRR1CNpNn%2FSrKBQhqEAMaDDQzNTc2MzIxMDc3NiIMruux8t46K%2BneMfaTKqcFWm2phwGzFHsoAmROZws0ZVN3VcP4O2KyE21Yukb3csuEXmmdNsIsvCVONDVYZv0ybanV0F4chxesXXVpQYgiDPLbQuUoP1wXeFfZMp2wYWTiva2vVSrwaGUlL%2BG5S%2FHLbyA2%2Fq3FjoN5G%2BKbVVzBvbaOWmYbDjYbNP6Xe2dADLl3OWdpwJIMug8RC6DR7nTsGZIUduz2VyMjrji9qQiSRV0YC716A5fPbqWKuRD45ncwDi0cL9qBeDgIY5hhHpni7ycP2VwWHpXj4IP77Veb%2BaPy9Q2K3uLR49jMA%2BLMgyweXNzYKNoJAiMCIxffOWL1NVSjgyPh84X5LS9gpUsA8CJS%2FsJT88gIbGe1ahjesJdcjcxrsfQ7ofIVBroLp0%2BDNOuclVhLsxVo89f2M317X0pi026W0tTuADzKSPyBp8GafEDLv1GVKGCpEGnURO3itGaeRwTTw%2F8HTPZFDzPWTdzCZ04jhxHnHKwJE4O0JHTsbSqTHE0BL7PYOFoLzSOCb8uapd11KJB9E%2FPp1pCMEYeuK7VDDRsnK95sJAJR%2BaKc5dCa0IVxj0Udz75gNnhDRI2ve3%2B0tWXyDsggsAL8fiQW%2FLCu0VHNikbP6JRlJhrrCoXUvrxgF6e4ow5cMOj5LoDaOUft0Zk6lWGqnAN1MZr614v%2BevcR9hyHG6iry1Nc8%2BWWccPOTTvfquB26pUm41WDKHivRSp9aWPjXO8cwb4LE1%2BtIHBbE2iz0Tn9Q29A9lfL%2BoWiFkCf3u3RdLkQKHTF4u%2BlP34b990K0FLyZ1w%2BZXBC766BcWCTvLNcYcFCQsKXe%2FQh7ZjG53cfpDNRcsmEx3Rkn1tUnWi29VwNZvA6rrRnXKqVHmseuhc6LKiq02L%2BLsxzX0dnT8gclQ1GV6tn5yRlIjCq5rq5BjqxAdaR0tgkJsd1FPGf5SOF5yftWoIhA4rR8zT7aWadhmeqqNzCV7vF7imksZpexClcFBX7%2FlwCKhyEWFpxTJ847ZuZhNDwsda8My4XDzFaRvuQESoadSzy0rrMShdtuQGZ22HN7QQl4MFR3hDXDa2Emn%2Bwi6CkCtdXdTo2N3yZ0bHcJ0Ilp8qbBY%2FkSuJbYOL6jDbj6lPKxcXlFA%2FCSD7vr1PFh1twMrOqkIq71DVH70kFUA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241109T011537Z&X-Amz-SignedHeaders=host&X-Amz-Expires=900&X-Amz-Credential=ASIAWK5MC5IMPTBRMNAV%2F20241109%2Fap-northeast-2%2Fs3%2Faws4_request&X-Amz-Signature=9f1538580585d13f002bde87c355a04f67bdb6eb9ce2822f5ab5ad649ee9cb86\n",
            "16590\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import boto3\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_transcription_result(job_name):\n",
        "    job = transcribe.get_transcription_job(\n",
        "        TranscriptionJobName=job_name\n",
        "    )\n",
        "\n",
        "    transcript_uri = job['TranscriptionJob']['Transcript']['TranscriptFileUri']\n",
        "\n",
        "    print(transcript_uri)\n",
        "    # Parse S3 URI to get bucket and key\n",
        "    from urllib.parse import urlparse\n",
        "    parsed_uri = urlparse(transcript_uri)\n",
        "    bucket = parsed_uri.netloc\n",
        "    key = parsed_uri.path.lstrip('/')\n",
        "\n",
        "    # Download the transcription file\n",
        "    response = requests.get(transcript_uri)\n",
        "    if response.status_code == 200:\n",
        "        transcript = response.json()\n",
        "\n",
        "        # Extract the transcription text\n",
        "        transcription_text = transcript['results']['transcripts'][0]['transcript']\n",
        "        return transcription_text\n",
        "    else:\n",
        "        return f\"Failed to download transcript. Status code: {response.status_code}\"\n",
        "\n",
        "transcription = get_transcription_result(job_name)\n",
        "print(len(transcription))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6R4PH_9gYbx"
      },
      "outputs": [],
      "source": [
        "with open(f\"{video_title}.txt\", \"w\") as text_file:\n",
        "    text_file.write(transcription)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kDG8EmI0dtc",
        "outputId": "97fe1bb0-d8d9-456f-ca73-751b809b88f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Google DeepMind, the podcast with me, your host, Professor Hannah Fry. Now, when we first started thinking about making this podcast, way back in 2017, DeepMind was this relatively small focused AI research lab. They’d just been bought by Google and given the freedom to do their own quirky research projects from the safe distance of London. Well, how things have changed? Because since the last season, Google has reconfigured its entire structure, putting AI and the team at DeepMind at the core of its strategy. Google DeepMind has continued its quest to endow AI with human level intelligence, known as artificial general intelligence or AGI. It has introduced a family of powerful new AI models called Gemini, as well as an AI agent called Project Astra that can process audio, video, image and code. The lab is also making huge leaps in applying AI to a host of scientific domains, including a brand new third version of AlphaFold which can predict the structures of all of the molecules that you will find in the human body, not just proteins. And in 2021, they spun off a new company, Isomorphic Labs, to get down to the business of discovering new drugs to treat diseases. Google DeepMind is also working on powerful AI agents that can learn to perform tasks by themselves using reinforcement learning and continuing that legacy of AlphaGo’s famous victory over a human in the game of Go. Now, of course, you’ll all have been following this podcast since the beginning. You’ll all be familiar with the stories behind all of those changes. But just in case you are coming to us fresh, welcome. You can find our 1st award winning previous seasons on Google DeepMind’s YouTube channel or wherever you get your podcasts. They also those episodes go into detail about a lot of the themes that we’re gonna hear come up over and over again from the people here, like reinforcement learning, deep learning, large language models, and so on. So have a listen. They are really good, even if we do say so ourselves. Now all of the newfound attention on AI since the last series does mean that there are quite a few more podcasts out there for you to choose from. But on this podcast, in just the same way as we always have, we want to offer you something a little bit different. We want to take you right to the heart of where these ideas are coming from, to introduce you to the people who are leading the design of our collective future. No hype, no spin, just compelling discussions and grand scientific ambition. So with all of that in mind, I am here with the DeepMind co founder and now CEO of Google DeepMind, Demis Hassabis. So with all of that in mind Yes. Do I have to call you sir Demis now? You don’t know. Absolutely not. Okay. Well, Demis, welcome to the podcast. Thank you. Thank you very much for being here. Okay. I want to know, is your job easier or harder now that there has been this explosion in public interest? I think it’s double edged. Right? I think it’s harder because there’s just so much scrutiny, focus, and actually quite a lot of noise in the whole field. I I actually preferred it when it was less people and maybe a little bit more focused on the science, but it’s also good because it shows that, the technology is ready to impact the real world in many different, you know, ways and impact people’s everyday lives in positive ways. So I think it’s exciting too. Hey. Have you been surprised by how quickly this has caught the public’s imagination? I mean, I I guess you would have expected that eventually Yes. People would have got on board. Yes. Exactly. So it’s we at some point, you know, those of us who’ve been working on it for like us for for many years now, you know, even decades. So I guess at some point, the general public would wake up to that fact and and effectively everyone’s we’re starting to realize how important AI is gonna be. But it’s been quite surreal still to see that actually come to fruition and and for that to happen. And I guess it is the advent of the chatbots and language models because everyone, of course, uses language, everyone can understand language. So it’s an easy way for the general public to understand and maybe measure where AI has got to. I heard you describe these chatbots as though they were unreasonably effective, which I really like. And, actually, later in the podcast, we are gonna be discussing Transformers, which was the big, breakthrough, I guess, the big advance that that gave us those tools. But but tell me first, what what do you mean by unreasonably effective? What I mean by it is I suppose if one were to wind back 5, 10 years ago and you were to say, what we’re gonna the way we’re gonna go about this is, you know, build these amazing architectures and then scale from there and not necessarily crack specific things like concepts or, abstractions. So these are a lot of debates we would have 5, 10 years ago is you need a special way of doing abstractions. The brain certainly seems to do that. But yet somehow the the systems, if you give them enough data, I. E. The whole Internet, then, they do seem to learn this and generalize from those examples, not just rote memorized, but actually, somewhat, understand what they’re processing. And it’s sort of a little bit unreasonably effective in the sense that, like, I don’t think anyone would have thought that it would work as well as it has done, say, 5 years ago. Yeah. I suppose it is a surprise that that that things like conceptual understanding and and abstraction have emerged rather than been been Yes. And and we would have been probably, we discussed last time, things like concepts and grounding, you know, grounding language in real world experience, maybe in simulations or as robots, embodied intelligence, would have been necessary to really understand, the world around us. And, of course, these systems are not there yet. They make lots of mistakes. They don’t really have a model of the world, a proper model of the world. But they’ve got a lot further than one might expect just by learning from language. I guess we probably should actually say what grounding is for those who haven’t, listened to series 1 and series 3 because this was a big thing. I mean, we were talking about this a lot about why you need so do you wanna just give us an overview about grounding? Grounding, is when, you know, that that was one of the reasons the the systems that were built in the eighties nineties, the classical AI systems built at places like MIT. There were big logic systems. So you could imagine them as huge databases of words connected to other words. And the problem was you could say something, a dog has legs. Right? And that would be in the database. But the problem was as soon as you showed a picture of a dog, it had no idea that collection of pixels was referring to that symbol. And that’s the grounding problem. So you have this symbolic representation, this abstract representation, but what does it really mean in the real world, in the messy real world? And and then, of course, they try to fix that but you never get that quite right. And instead of that, of course, today’s systems, they’re they’re directly learning from the data. So in a way, they’re forming that connection from the beginning. But the interesting thing was is that, you know, if you learn just from language, in theory, they should be missing a lot of the grounding that you need. But it turns out that a lot of it is inferable somehow. Why in theory? Well, because where is that grounding coming from? These sys at least the first kind of large language models Don’t exist in their real world. They don’t exist in their real world. They’re not connected to simulators. They’re not connected to robots. They don’t have any access to even they weren’t multimodal to begin with either. They don’t have access to the to to the visuals or anything else. It’s just purely they live in language space. So they’re living in a they’re learning in an abstract domain. So it’s pretty surprising they can then infer some things about the real world from that. Which makes sense if the grounding gets in by people interacting with this system and saying, that’s a rubbish answer. That’s a good answer. So for sure, part of that, if if the question that they’re getting wrong, the early versions of this, was, up to due to grounding missing, you know, actually, the real world dogs bark in this way or whatever it is, and it’s answering it incorrectly, then that feedback will correct it. And part of that feedback is from our own grounded knowledge. So some grounding is seeping in like that for sure. I remember seeing a really nice example about, crossing the English Channel versus walking across the English Channel. Right? Exactly. Those kinds of things. And if it answered wrong, you would tell it it’s wrong, and then it would have to sort of slightly figure out that you, you know, you can’t walk across the channel. Okay. So some of these properties that have that have emerged that weren’t necessarily expected to be, I kind of wanna ask you a little bit about hype. Do you think that that that where we are right now, how how things are at this moment Yes. Is overhyped or underhyped Yeah. Or is it just hyped perhaps in the wrong direction? Yeah. I think it’s more the latter. So I would say that, in the near term, it’s hyped too much. So I think people are claiming it can do all sorts of things at Calm. There’s all sorts of, you know, startups and VC money chasing crazy ideas that don’t you know, they’re just not ready. On the other hand, I think it’s still Coming from you, Demis. That’s a lot of people don’t mind me saying I know. But it in 2010. Exactly. Exactly. But but, you know, I think it’s still underhyped or perhaps underappreciated still, even now, what’s gonna happen when we get to AGI and post AGI. I still don’t feel like, that’s that’s people have quite understood how enormous that’s gonna be and therefore the sort of responsibility of that. So it’s sort of both, really. I think it’s it’s a little bit overhyped in the in the in the near term, at the moment. We’re kinda going through that cycle. I guess, though okay. So in terms of all of these potential startups and VC funding and so on, you who have lived and breathed this stuff for for, as you say, decades Yeah. Are are very well placed to spot which ones are are realistic goals and which ones aren’t. But for for other people, how can they distinguish between what’s real and and what isn’t? Yeah. Well, look, I think you need to look at, obviously, you’ve gotta do your technical due diligence, have some understanding of the technology and the and the latest sort of trends. I think also look at perhaps the, you know, the background of the people saying it, how technical they are. Have they just arrived in AI, like, last year from somewhere else? I don’t know. They were doing crypto last year. You know, these might be some clues that that that perhaps, you know, they’re jumping on a bandwagon, and it doesn’t mean to say, of course, they they could still have some good ideas and they many will do, but it’s a bit more, lottery ticket like, should we say. And I think that always happens when there’s a ton of, attention suddenly on a place and it’s obviously then the money follows that, and everyone feels like they’re missing out, and that creates a, a kind of, opportunistic, shall we say, environment, which is a little bit opposite to the people those of us who’ve been in for decades in a kind of deep technology, deep science way, which is ideally the way I think we need to carry on going, as we get closer to AGI. Yeah. And I guess one of the big things we’re gonna talk about in this series is Gemini Yes. Which really comes from that very deep science, approach, I guess. Yeah. In what ways is Gemini different from from the other large language models that are released by other labs? So from the beginning with Gemini, we wanted it to be multimodal from the start so it could, you know, process not just language but also audio, video, image, code, any modality really. And the reason we wanted to do that was, firstly, we think that’s the way to get these systems to actually understand the world around them and build better world models. So actually still going back to our grounding question earlier, still building grounding in but in the but piggybacking on top of language this time. And so that’s important. And we also had this vision in the end of having a universal assistant, and and we prototyped something called Astra, which I’m sure we’ll talk about, which, understands not just what you’re typing but actually the context you’re in. And if you think about something like a personal assistant or digital assistant, it will be much more useful if the more context it understood about what you’re asking it for or the situation that you’re in. So we always, thought that would be a a a much more useful type of system, and so we built multimodality in from the start. So that was one thing, natively multimodal. And then, at the time, that was the only model doing that. So now the other models are trying to catch up. And then the other big innovations we had are on memory, so, like, long context. So actually holding in mind, you know, a 1,000,000 or 2,000,000 now tokens, you can think of them as more or less like words in mind. So you can, you know, give it war of peace or or even a whole because multimodal, a whole video now, a whole film and and, or lecture and then get it to answer questions or find you things within that video stream. Okay. Project Astra, that’s the the new universal AI agent, the one that can take in video and and audio data. Google IO, I think you used the example of how Yeah. Astra could help you remember where you left your glasses, for instance. So I wonder though about the lineage of this stuff. Because is this just a a kind of fancy, advanced version of those old Google Glasses? Of course, Google have a long history of developing glass type devices, actually back to, I think, 2012 or something, so they’re way ahead of the curve. But, maybe they it was just missing this kind of technology so you could actually understand a smart agent, a smart assistant that could actually understand what it’s seeing. And so we’re very excited about that digital assistant to, you know, to go around with you and understand the world around you. So it seems like a really you know, when you use it, it feels like a really natural use case. Okay. I wanna rewind a tiny bit to sort of the the the the start of Gemini because because it came from 2 separate parts of the organization. Yes. So we, actually, last year, we combined our 2 research divisions at at at Alphabet. So obviously, the old DeepMind and then brain, Google Brain, into one we call a super unit, bringing all the talent together that we you know, amazing talent we have across the company, across the whole of Google into 1 unified, unit. And what it meant was is that we combined all the best knowledge that we had, from, all the research we were doing, but especially on language models. So we had Chinchilla and Gopher and things like that, and they were building things like PaLM and Lambda and early language models. And they had different strengths and weaknesses, and we pulled them all together into what became Gemini as the first lighthouse project that the combined group would would would, output. And then the other important thing is, of course, is was bringing together all the compute, as well so that we could, you know, do these really massive training runs, and actually pull the compute resources together. So it’s been great. In in a lot of ways, I mean, the focus of Google Brain and DeepMind was was slightly different. Yes. Is that fair to say? Yeah. So I I think we it it it was. I mean, we were obviously focused on the both of us on the frontiers of AI, and and there was a lot of collaborations already on a kind of individual researcher level, but maybe not on a strategic level. Obviously, now, the combined group, Google DeepMind, I kind of describe it as we’re the engine room of Google now. But it’s it’s worked really well. I think there were a lot more similarities actually in the way we were working, than there were differences, and we’ve continued to keep and double down our strengths on things like fundamental research. So, you know, where does the next transformer architecture come from? We want to invent that. Obviously, we you know, Google Brain invented the previous one. We combine it with deep reinforcement learning that we pioneered. And I still think more innovations are gonna be needed, and I would back us to do that just as we’ve done in the past 10 years, you know, collectively, both Brain and and DeepMind. So, it’s been exciting. I wanna come back to that that merge in in a moment. Yeah. But I think just just sticking on Gemini for a second, how good is it? How does it compare to other models? Yeah. Well, I think it’s you know, some of the benchmarks are not we’re the problem is that we need more I think there’s one thing the whole field needs is much better benchmarks. Yeah. I’m using time. Abilities. Well, there are some well known benchmarks, academic ones, but they’re kinda getting saturated now and they don’t really differentiate between the the the nuances between the different top models. I would say there’s sort of 3 models that are kind of, at the top of the frontier. So it’s, Gemini for Mars, OpenAI’s GPT, of course, and then Anthropic with their Claude models. And then, obviously, there’s a bunch of other, good models too that, you know, people like Meta and Mistral and others build. And they’re differently good at different things. It depends what you want. You know, coding, perhaps that’s Claude and reasoning, maybe that’s GPT. And then memory stuff, long context and multimodal understanding, that would be Gemini. Of course, we’re continuing to all of us are improving our models all the time. So, you know, given where we started from, which Gemini as a project only existed for a year, you know, obviously based on some of our other projects, I think our trajectory is very good. So, you know, when we talk next time, we should, you know, hopefully be, you know, right at the forefront. Because there is there is still a way to go. I mean, there are still some things that these models aren’t very good at. Yes. For sure. And and, actually, that’s the big debate right now. So this last set of things kind of emerged from the technologies that were, you know, invented 5, 6 years ago. The question is, is they’re still missing a ton of things? So they they’re factuality, you know, they hallucinate, as we know. They’re also not good at planning yet. They’re Planning in what sense? I mean Well, like kind of long term planning. So they can’t problem solve, something long term. You give it an objective. They can’t really do actions in the world for you. So they’re they’re very much like passive q and a systems. You know, you put the energy in by asking the question and then they give you some kind of response. But they’re not able to solve a a a problem for you. You can’t say something like if you wanted as a digital assistant, you might wanna say something like, you know, book me that holiday in Italy and all the restaurants and the museums and whatever and and, you know, it knows what you like. But then it goes out and books the flights and all of that for you. So it it can’t do any of that. But I think that’s the next era. These sort of more agent based systems, we would call them, or agentic systems that, have agent like behavior. But, of course, that’s what we’re expert in. That’s what we used to build with all our game agents, AlphaGo and all of the other things we’ve talked in about in the past. So a lot of what we’re doing is bringing to kind of marrying that work, that we’re sort of, I guess, famous for and then, with the new large, multimodal models. And I think that’s going to be, you know, the next generation of systems. You can think of it as combining AlphaGo with Gemini. Yeah. Because I guess AlphaGo was very, very good at planning. Yes. It was very good at planning, of course, only in the domain though of games, and so we need to sort of generalize that, into the, you know, the the general domain of everyday, workloads and language. You mentioned a minute ago how, Google DeepMind is now sort of the engine room of of Google. I mean, that is quite a big shift since since I was last year last couple of years ago. Is Google taking quite a big gamble on you? Yeah. Well, I guess so. I mean, I think Google have always, understood the importance of AI. You know, we’ve been Sundar, when he took over as CEO, said, that Google was an AI first company, you know, and we we discussed that very early on in his tenure, and he he saw the potential in AI as the next big paradigm shift after mobile and Internet, you know, but bigger than those things. But then I think maybe in the last year or 2, we’ve really started living what that means, not just from a research perspective but also from products and and other things. So, it’s very exciting, but I think it’s the right bet for us to kind of coordinate all of our talents together and then, push as hard as, you know, as possible. And then how about the other way around? Because I guess from DeepMind having that very strong research and, like, science focus, does becoming the engine room for Google now mean that you have to care much more about commercial interest rather than the the sort of purer stuff that you Yeah. Well, we do definitely have to come come, worry more about and and and and it’s in our remit now, the commercial interests and but, actually, there’s sort of a couple of things to say about that. First of all, we’re continuing on with our science work and AlphaFolds and, you know, you just saw AlphaFold 3 come out and, you know, we’re we’re doubling down on our investments there. That’s, I think, a unique thing that we do at at Google DeepMind now. You you know, and even our competitors point at those things as sort of, you know, universal goods, if you like, that come out of AI. And that’s going really well and we spun out isomorphic to to do drug discovery. So, it’s very exciting. That’s all going really well. And so we’re gonna continue to do that. And then what was all our work on climate and all of these things And then but then we’re quite a large team, so we can do more than one things at once. We’re also building our large models, Gemini and etcetera. And then, we have a product team that we’re building out that is going to, you know, bring all this amazing technology to all of the surfaces that Google has. So it’s an incredible sort of, privilege in a way to have that there to plug in all of our stuff. And, you know, we invent something. It immediately can become useful to a 1,000,000,000 people. And so that’s really motivating. And, actually, the other thing is is there’s less, it’s there’s a lot more convergence now between the technology you would need to develop for a product to have AI in it and what you would do for pure AGI research purposes. So, there’s not really you know, 5 years ago, you’d have had to build some special case AI for a product. Now you can branch off your main research. And, of course, you still need to do some things that are product specific, but maybe it’s only 10% of the work. So there’s actually not that tension anymore between, what you would develop for an AI product and what you would develop for trying to build AGI. It’s it’s it’s 90%, I would say, the same the same, research program. So, and then finally, of course, if you do products and you get them out into the world, you learn a lot from that, which and and people using it and you learn a lot about, oh, your internal metrics don’t quite match what people are saying, you know, so then you can update that. And that’s really helpful for your research. Absolutely. Well, okay. I mean, we are gonna talk a lot more in this podcast about those breakthroughs that have come from applying AI to science. But I wanna ask you about that tension that there is between knowing when the right moment is to to release something to the public. Because internally at DeepMind, those tools like large language models were being used for research rather than being seen as a potentially commercial thing. Yeah. That’s right. So we we’re you know, as as you know, we’ve always taken responsibility incredibly seriously here and safety, right from the beginning, you know, way back when we started in 2010 and before that. And Google, then adopted some of our, basically, ethics charter effects into their AI principles. So we’ve always been well aligned with the whole of Google and and wanting to be responsible about about deploying this as one of the leaders in this space. And so it’s been interesting now starting to ship real products with Gen AI in them. You know, actually, there’s a lot of learning that is going on, and we’re learning faster, which is good because we’re relatively low stakes here with the current technology. Right? So it’s not that powerful yet. But as it gets more powerful, we have to be more careful. And that’s just learning about the product teams that, you know, in other groups, learning about how to test Gen AI technologies. It’s different from a normal piece of technology because it doesn’t always do the same thing. People can it’s almost like, testing an open world game. It’s almost infinite what you can try and do with it. So it’s it’s sort of interesting to figure out how do you do the red teaming on it. So red teaming in this case being where you’re competing against yourselves? Yes. So red teaming is when you set up a specific separate team from the from the team that’s developed the technology to stress test it and try and break it in any way possible. You know, you actually need to use tools, to automate that because nobody can red team. Even if you had thousands of people doing it, that’s not enough compared to billions of users when you put it out there. They’re gonna try all sorts of things. So it’s, it’s kind of interesting to take that learning and then improve our processes so that, you know, our future launches will be as smooth as possible. And I think we gotta do it in stages, you know, where there’s experimental phase, then a kind of, you know there’s closed beta and then and then launch. A little bit, again, like we used to launch, our games back in the day. So you sort of and learn at each step of the way. So there’s a, you know and and then the other thing we gotta do, I think we need to do more on, is use AI itself to, help us internally with with red teaming and and and actually spotting some errors automatically or trio triaging that so that the then the human you know, our our our kind of developers and human testers can actually, focus on those hard hard cases. So there’s something really interesting there about how you’re just in a much more probabilistic space here. Right? And and and then if there’s even a very small chance of something happening Yeah. If you have enough tries, eventually, something will go wrong. And I I guess there have been a couple of mistakes that you know, public mistakes. Yeah. So that’s why, you know, I think that, as I mentioned, that product teams are just getting used to the sorts of testing. They, you know, they tested these things and but but they have this stochastic nature, probabilistic nature. So in fact, a lot of cases where, you know, if it was a normal piece of software, you could say, I I tested 99.999 percent of things. So then extrapolates. Yes. And then it’s enough because it’s like, you know, there’s no way of exposing the flaw that it has if it has 1. But that’s not the case with these generative systems. You know, they can do all sorts of things that are a little bit, left field or out of the box, out of distribution in a way from what you’ve seen before. If someone clever or adversarial decides to it’s almost like a hacker, decides to, test push it in some way. And it could even be I mean, it’s so combinatorial. It could even be with all the things that you’ve happened to have said before to it and then you you then it’s in some kind of peculiar state which then or it’s got its memories filled out with particular thing and then it that’s why it outputs something. So there’s a lot of, complexity there and and and but it’s not infinite. So there’s there’s ways to deal with it. But, it’s it’s just a lot more nuanced than than launching normal technology. I remember you saying I think it was, like, in the first, first time I interviewed you about how actually you have to think that this is a completely different way of computing. You kind of have to move away from the sort of the things that we completely understand, the deterministic stuff Mhmm. Into this much more messy Yeah. Like, probabilistic Yeah. Error ridden Yeah. Place. Yeah. As well as your testers, do you think the public slightly has to shift its mindset on the type of computing that we’re doing now? I think so because and maybe that’s something we you know, another thing interestingly that that that, you know, we’re thinking about is actually putting out a kind of principles document or something before you release something to sort of show what is the expectation from this system. You know, what’s it designed for? What’s it useful for? What can’t it do? And I think, you know, there is some sort of education there needed of, like, you you’ll be able to find it useful if you do these things with it, but don’t try and use it for these other things because it won’t work. And, I think that that’s something that, you know, we’re we, you know, we need to get better at clarifying as a field and then probably users need to get, more experienced on. And, actually, this is interesting. This is probably why chatbots themselves would came a little bit out of the blue. Right? Even, obviously, ChatGPT, but even to OpenAI, it surprised them. And and we had our own chatbots and Google had theirs. And one of the things was is we were looking at them and we were looking at the all the flaws they still had. Right? And they still do. And it’s like, well, it’s getting these things wrong and it sometimes does, you know, hallucinates and blah blah blah. And there’s so many things. But then what we didn’t realize is, actually, there’s still a lot of very good use cases for that, even now that people find very valuable. You know, summarizing documents and really long things or writing, you know emails. Or awkward emails or or mundane, you know, forms to be filled in. And and there’s all these use cases which actually people, don’t mind if there’s some small errors. They can fix them easily and saves a huge amount of time. And I guess that was the surprising thing. They sort of discovered people discovered when you put it in the hands of everyone. There were these there were there were actually these valuable use cases even though this the the systems were flawed in all of these ways we know. Well, okay. So I think that sort of takes me on to the next question I wanna ask, which is about open source. Mhmm. Because because when things are in the hands of people, as you as you mentioned, really, extraordinary things can happen. And I know that DeepMind in the past has open sourced lots of its research projects. But but it feels like that that that’s slightly changing now as we go forward. So just tell just tell me what your stance is on open source. Yeah. Well, look. We we’re huge supporters of open source and open science. As you know, we’ve we’ve, I mean, we’ve given away and published almost everything we’ve done, you know, if you collectively including, like, things like Transformers, right, and AlphaGo, we published all these things in Nature and Science. AlphaFold was, open source as as we covered last time. And these are all good choices, and you’re absolutely right. That’s the reason that all works is because that’s the way, technology and science advances as quickly as possible, by sharing information. So almost always that’s a universal good to do it like that and that’s how science works. The only exception is when your and and a AI, AGI and AI, powerful AI, does fall into this is, we have a dual purpose technology. Right? And so then the problem is is that you want to enable all the good use cases and and all the genuine scientists who are acting in good faith and so on, technologists, to to build on the ideas, critique the ideas, and so on. That’s the way, you you know, society advances the quickest. But the problem is, how do you restrict access at the same time for bad actors who would take the same systems, repurpose them for bad ends, misuse them, you know, weapon systems, who knows what. And, you know, those general purpose systems can be repurposed like that. And it’s okay today because I don’t think the systems are that powerful. But in 2, 3, 4 years’ time, especially when you start getting agent like systems or agentic behaviors, then I think, you know, if it’s something’s misused by someone or perhaps even a rogue nation state, there could be serious harm. So then I think that as a as I don’t have a solution to that, but as a as a community, we need to think about what does that mean for open source. Perhaps the frontier models need to have more checks on them, and then only after they’ve been out for a year or 2 years, then they can get open sourced. That’s sort of the model we’re following with because we have our own open models of Gemini called Gemma, because they’re smaller, so they’re not frontier models. So usually their so their capabilities are very useful still to the developer because they’re also easy to run on a laptop or because they’re small in numbers of parameters. But, but the capabilities they have are well understood at this point, right, because they’re not frontier models. So it’s just not as powerful as the latest, say, Gemini, you know, 1.5 models. So I think that’s probably the approach that we’ll end up taking is, we’ll have open source models, but they’ll be lagging, you know, maybe 1 year behind the the the most cutting edge models just so that those model we can really assess out in the open, you know, by by users what those models can do, the frontier ones can do. And you can really, I guess, test those those boundaries of the Yeah. And we’ll see what those are. The problem with open source is if something goes wrong, you can’t recall it. Right? With a proprietary model, if your bad actor starts using it in a bad way, you can just you can just sort of close the tap off, you know, in the limit, you could switch it off. Right? But, once you open source something, there’s no pulling it back. So it’s a one way door. So you should be very, very sure when you do that. Is it definitely possible to contain an an AGI, though, within the sort of walls of an of an organization? Well, that’s a whole separate question. I don’t think we know how to do that right now. So that’s that’s when you start talking about AGI level powerful, like human level AI. What about intermediary? Well, intermediary, I think we have that we have good, ideas of how to do that. So, you know, one would be things like, secure sandboxing. So you test that’s what I’d wanna test the agent behaviors in is in a game environment or a version of the Internet that’s not quite fully connected. Right? So, there’s a lot of, security work that’s done and known, you know, in this space and in Fintech and other places. So we’d probably borrow those ideas and and then build those kinds of systems and that’s how we would test the early prototype systems. But, that’s we also know that’s not gonna be good enough to contain an AGI, something that’s potentially smarter than us. So I think we’ve gotta understand those systems better so that we can design the protocols for an AGI. When that time comes, we’ll have better ideas for how to contain that potentially also using AI systems and tools to monitor, the next versions of the AI system. So one, the subject of safety then, because I I know that you were a very big part of the AI Safety Summit at Bletchley Park in 2023, which is, of course, hosted by the UK government. And and from the outside, I think a lot of people just say the word regulation as though it’s just gonna come in and and fix everything. But what is your view on how regulation should be structured? Well, I think it’s great that governments are getting up to speed on it and involved. I think that’s one of the good re things about the the recent explosion of interest is that, of course, governments are paying attention. And I think it’s been great. UK government, specifically, who I’ve talked to a lot and US as well, they’ve got very smart people in the civil service staff that are, understand the technology now to to a good degree. And it’s been great to see the AI safety institutes being set up in the UK and US, and I think many other countries are gonna follow. So I think these are all good precedents and protocols to settle into, again, before the stakes get really high. Right? So it’s this is a sort of proving stages again as well. And I do think international cooperation is gonna be needed ideally around things like regulation and guardrails and deployment norms. So, because AI is a digital technology, very much so, you know, it doesn’t it’s hard to contain it within national boundaries. Right? So if if the UK or Europe does something, but or even the US, but China doesn’t, does that really help the world as opposed you know, when we start getting closer to AGI? Not really. So I think my my view in it is you’ve got to be, because the technology is changing so fast, we’ve got to be very nimble and and light footed with regulation so that it’s easy to adapt it to where the latest technology is going. If you’d regulated AI 5 years ago, you’d have regulated something completely different to what we see today, which is gen AI. And but it might be different again in 5 years. It might be these agent based systems that are the ones that carry the highest risk. So right now, I would, you know, recommend to sort of beef up existing regulations in in domains that already have them, health, transport, so on. I think, you know, you can update them for AI for an AI world just like they were updated for mobile and Internet. That’s probably the first thing I do while doing a watching brief on, you know, and making sure you understand and test the frontier systems. And then as things become clear and sort of more clearly obvious, then, start regulating around that. You know, maybe in a couple of years’ time would make sense. One of the things we’re missing is, again, the benchmarks, the right test for capabilities that what we’d all wanna know, including the industry and the field, is at what point are capabilities posing some sort of big risk? And and there’s no answer to that at the moment. Right? Beyond what I’ve just said, which is agent based capabilities is probably a next threshold. But there’s no agreed upon test for that. You know, one thing you might imagine is, like, testing for deception, for example, as a capability. You really don’t want that in the system, because then you can’t rely on anything else that it’s reporting. Right? So, that would be my number one, emergent capability that I think, you know, would be good to test for. But there’s many. You know, ability to achieve certain goals, ability to replicate, and there’s quite a lot of work going on on this now. And I think the safety institutes, which are basically sort of government, agencies, I think they’re working I think it’d be great for them to do a lot you know, to push on that as well, as well as the labs, of course, contributing what we know. I wonder, in this this picture of the world that you’re that you’re describing, what’s the place for institutions in this? I mean, if we get to the stage where we have AGI that’s kind of supporting all scientific research Mhmm. Is there still a place Mhmm. For great institutions? Yeah. I think so. Look, I well, there’s there’s there’s sort of the stage up to AGI, and I think that’s gotta be a cooperation between civil society, academia, government, and and the industrial labs. So I think I really believe that’s the only way we’re gonna get to the to the to the sort of final stages of this. Now if you’re asking after AGI happens, you know, that maybe that is what you’re asking, then AGI, of course, one of the reasons I’ve always wanted to build it is then we can use it to start answering some of the biggest, most fundamental questions about the nature of reality and physics and all of these things and consciousness and so on. It depends, you know, what form that takes, whether that’ll be a human, expert combination with AI. I think that will be the case for a while, in terms of discovering the next frontier. So, like, right now, these systems can’t come up with their own conjectures or hypotheses. They can help you prove something, and I think we’ll be able to prove, you know, gold get gold medals on International Math Olympiad, things like that. But, maybe even solve a famous conjecture. I think we’re that’s within reach now, but not they don’t have the ability to come up with Riemann hypothesis in the 1st place, right, or general relativity. So that’s really was always my test for, maybe a true artificial general intelligence is it will be able to do that or invent Go. You know? And and so we don’t have any systems. We don’t really know how even probably, you know, know how we would design, in theory even, a system that could do that. You know the computer scientist, Stuart Russell? So he told me that he was a bit worried that once we get to ATI, it might be that we all become like the royal princes of the past. You know, the ones who never had to ascend the throne or do any work, but just got to live this life of unbridled luxury and have no purpose? Yeah. So that’s that is the interesting question is it maybe it’s beyond AGI. It’s more like artificial superintelligence or something. Sometimes people call it ASI. But then we should have, you know, radical abundance. And assuming we, you know, make sure we distribute that, you know, fairly and equitably, then we will be in this position where, you know, we’ll have more freedom to choose what to do. And, and then meaning will be a big philosophical question. And I think we’ll need philosophers, perhaps theologians even, to start thinking as social scientists, that they should be thinking about that now. What what brings meaning? I mean, I still think, there’s, of course, self actualization, and I don’t think we’ll all just be sitting there meditating. But but but but maybe we’ll be playing computer games. I don’t know. But is that a bad thing even or or not? Right? Who who knows? Think the princes of the past came off particularly well. No. Traveling the stars. But then there’s also, you know, extreme sports people do. Why do they do them? I mean, you know, climb Everest, all these. I mean, they’ll be you know? But I think it’s gonna be very interesting, and and that I don’t know. But that’s that’s kind of what I was saying earlier about the it’s under appreciated what’s gonna happen, you know, going back to the hype, near term versus far term. So if you wanna call that hype even, it’s it’s definitely under hyped, I think, the amount of transformation that will happen. I think I think it will be very good in the limit. We’ll cure lots of diseases and all diseases, you know, solve our energy problems, climate problems. But then the next question comes is is is there meaning? So bringing us back, like, slightly closer to to AGR rather than than superintendents, I know that your big mission is to to build, artificial intelligence to benefit everybody. Yeah. But how do you make sure that it does benefit everybody? How do you include all people’s preferences rather than just the designers? Yeah. I think, you’ve gotta I think what’s gonna have to happen is I mean, it’s impossible to include all preferences in one system because by definition, people don’t agree. Right? We can see that in, unfortunately, in the current state of the world. Countries don’t agree. Governments don’t agree. We can’t even get agreement on obvious things like like dealing with the climate, situation. So I think it’s that’s very hard. What I imagine that will happen is that, you know, we’ll have a set of safe architectures, hopefully, that, personalized AIs can be built on top of. And then everyone will have, you know or or different countries will have their own preferences about what they use it for, what they deploy it for, what they you know, what can and can’t be done with them. But overall and that’s fine. That’s for everyone to individually decide or countries to decide themselves just like they do today. But as a society, we know that, there’s some provably safe things about those architectures. Right? And then you can let them proliferate and and so on. So I I I think that we’re gonna kind of gotta get through the eye of a needle in a way where, we gotta as we get closer to AGI, we probably gotta cooperate more ideally, ideally, internationally, and then get make sure we build AGIs in a safe architecture way, because I’m sure there are unsafe ways, and I’m sure there are safe ways of building AGI. And then once we get through that, then we can sort of open the funnel again and everyone can have their own personalized pocket AGIs, if they want. What a version of the future. Okay. But then in terms of the safe way to build it, I mean, are we talking about undesirable behaviors here that might emerge? Yes. Undesirable emergent behaviors, capabilities, the the deception is one example that that you don’t want, value systems. You know, we we gotta understand all of these things better. What kind of guardrails work, not circumventable? And there’s 2 cases to worry about. There’s the there’s bad uses by by bad, individuals or or nations, so human misuse, and then there’s the AI itself, right, as it gets closer to AGI doing going off the rails. So that and I think you need different solutions for those two problems. And so, yeah, that’s that’s what we’re gonna have to contend with as we get closer to, building these technologies. And also just going back to your benefiting everyone point, of course, what what I’m you know, we’re showing the way with things like AlphaFold and isomorphic. I think we could, you know, cure most diseases within the next decade or or 2 if, AI drug design works. And then there could be personalized medicines where it minimizes the side effects on the individual because it’s it’s mapped to the the person’s individual illness and their individual metabolism and so on. So these are kind of amazing things, you know, clean energy, renewable energy sources, you know, fusion or better solar power, all of these types of things. I think they’re all within reach, and then that would sort out water access because you could do desalination everywhere. So I just feel like, it’s gonna enormous good is gonna come from, these technologies, but we have to mitigate the risks too. And one way that you said that you would want to mitigate the risks was, that there would be a moment where you would basically do the scientific version of Avengers Assembled. Yes. Sure. Terrence Tower. Come on down. Co. Exactly. Bring them on down. Exactly. Is is that still your plan? Yeah. Well, I think I think I think so. I think if we can get the international cooperation, you know, I’d love there to be a kind of international CERN, basically, for AI, where you get the top researchers in the world. You know, look. Let’s focus on the final few years of this prod you know, AGI project and get it really right and do it scientifically and carefully and thoughtfully at every every step that the final sort of steps. I still think that would be the best way. How do you know when is the time to press the button? That’s all that’s the big question because you you can’t do it too early because you would never be able to get the buy in to do that. Mhmm. A lot of people would disagree. And today, people disagree with the risks. Right? You see very famous people saying there’s no risk, and then you have people like Geoff Hinton saying there’s there’s lots of risks. And, you know, I’m I’m I’m in the middle of that. I wanna just talk to you a bit more about neuroscience. How much does it still inspire what you’re doing? Because I noticed the other day that DeepMind had unveiled this computerized rat with a with an artificial brain that that helps to change our understanding of of how the brain controls movement. But in the first season of the podcast, I remember we talked a lot about how DeepMind takes direct inspiration from biological systems. Is that still the core of your your approach? No. It’s evolved now because I think we’ve got to a stage now in the last, I would say, 2, 3 years, we’ve gone more into an engineering phase, large scale systems, you know, massive, training architectures. So I would say that the influence of a of of neuroscience on that is a little bit less. It may come back in. So anytime where you need more invention, then you want to get as many sources as possible, and neuroscience would be one of those sources of of ideas. But when it’s more engineering heavy, then, I think that takes a little bit more of a backseat. So it may be more applying AI to neuroscience now like you saw with the virtual rat brain, and I think we’ll see that as we get closer to AGI using that to understand the brain. I think it’d be one of the coolest use cases for AGI and science. I guess this stuff kinda goes through phases of, like, the engineering challenge, the intervention challenge. So it’s done its part. It’s you know, for for now, and it’s it’s been great. And we still obviously keep a close track of it and take any other other ideas too. Okay. All of the pictures of the future that you’ve painted, are still anchored quite in reality. But I know that you’ve said that you really want, AGI to be able to peer into the mysteries of the universe Yes. Down at the Planck scale. Yes. Like, kind of subatomic Yes. Quantum worlds. Yeah. Do you think that there are things that we have not even yet conceived of Mhmm. That’s that might end up being possible? I’m talking wormholes here. Completely. Yes. I love wormholes to be possible. I I think we there is a lot of probably misunderstanding, I would say, still things we don’t understand about physics and then and the nature of reality. And, you know, obviously, the quantum mechanics and unifying that with, you know, gravity and all of these things. And there’s all these problems with the standard model. So I think there’s there’s and string theory, you know. I I mean, I just think There’s giant gaping holes in physics really. Physics all over the place. And if you’re you know, I talk to my physics friends about this. And there’s a lot of things that don’t fit together. I don’t really like the multiverse explanation. So I think that, it will be great to, come up with new theories and then test those on massive apparatus perhaps out in space, at these these tiny you know, the reason I’m obsessed with Planck scale things, Planck time, Planck space, you know, is is, because that seems to be the resolution of reality. Right? That in a way, that’s the kind of smallest quanta you can break anything into. So that feels like the kind of level you want to experiment on. If you have powerful, apparatus perhaps designed or enabled by having AGI and radical abundance, you need both. So to be able to afford to build those types of experiments. So resolution of reality. Yeah. What a phrase. Yeah. What so is it, like, the resolution that we’re at at the moment Yeah. Sort of human level is just an approximation of reality. That’s right. And then we know there’s the atomic level, wherein below that, the Planck level, which as far as we know, is the smallest resolution one can even talk about things. And so that, to me, would be the resolution one wants to experiment on to really understand what’s going on here. I wonder whether you’re also envisaging that there’ll be things that are beyond the limits of human understanding. AGI will help us to to uncover that, actually, we’re just not really capable of understanding. And then I sort of wonder, if if things are are unexplainable or un understandable, are they still falsifiable? Yeah. Well, look. I mean, these are great questions. I think there will be a potential for an AGI system to understand higher level abstractions than we can. So through again, through neuro going back to neuroscience, we know that, you know, it’s your prefrontal cortex that does that, and there’s sort of up to about 6 or 7 layers of of indirection, you know, one could take. You know, this person’s thinking this, and I’m thinking this about that person thinking this and so on, and then we we sort of lose track. But, I think an AI system could have an arbitrarily sort of large prefrontal cortex effectively. So you could imagine higher levels of abstraction and patterns that it will be able to see about the universe that we can’t really comprehend or hold in mind at once. And then I think the from a terms of explainability point of view, the way I think that is a little bit different to other philosophers who’ve thought about this, which is, like, we’ll be, like, closer to an ant and then the AGI, right, in terms of IQ. But I think that’s the way to think of it. I think, you know, it’s it’s we we are truing complete. So we’re sort of a, you know, a full general intelligence as ourselves, albeit a bit slow because we run on slow machinery, and we can’t, you know, infinitely expand our own brains. But, we can, in theory, given enough time and and memory, understand, anything that’s computable. And so it I think it’ll be more like, you know, Gary Kasparov or Magnus Carlsen playing an amazing chess move. I couldn’t have come up with it, but they can explain it to me why it’s a good move. So I think, that’s what an AGI system will be able to do. You said that DeepMind was a 20 year project. Yeah. How far through are we? Are you are you on track? I think we’re on track. Yeah. Crazily. Because usually 20 year projects stay 20 years away. Yeah. But, yeah, we’re a good way in now. And I think we’re 20 years is 2030 for HRT. Yeah. So I think I would the way I say it is I wouldn’t be surprised if it comes in the next decade. So I think we’re on track. That matches what you said last time. You haven’t updated your priors. Exactly. Amazing. Yep. Denis, thank you so much. Thanks. Absolute delight. Absolute delight as always. So fun to talk as always. Well, thank you. Okay. I think there are a few really important things that came out of that conversation, especially when you compare it to what Dennis was saying last time we spoke to him in 2022. Because there there have definitely been a few surprises in the last couple of years. The way that these models have demonstrated a genuine conceptual understanding is 1. This this real world grounding that came in from language and human feedback alone. We did not think that that would be enough. And then how interesting and useful imperfect AI has been to the everyday person. Demis himself there admitted that he had not seen that one coming. And that makes me wonder about the other challenges that we don’t yet know how to solve, like long term planning and agency and robust, unbreakable safeguards. How many of those, which we’re going to cover in detail in this podcast, by the way, are we gonna come back to in a couple of years and realize that they were easier than we thought? And how many of them are gonna be harder? And then as for the big predictions that Demis made like cures for most diseases or in 10 or 20 years or or AGI by the end of the decade or how we’re about to enter into an era of abundance, I mean, they all sound like Demis is being a bit overly optimistic, doesn’t it? But then again, he hasn’t exactly been wrong so far. You’ve been listening to Google Deep Minds, the podcast with me, professor Hannah Fry. If you have enjoyed this episode, hey, why not subscribe? We have got plenty more fascinating conversations with the people at the cutting edge of AI coming up on topics ranging from how AI is accelerating the pace of scientific discoveries to addressing some of the biggest risks of this technology. If you have any feedback or you want to suggest a future guest, then do leave us a comment on YouTube. Until next time.\n"
          ]
        }
      ],
      "source": [
        "with open(\"transcript.md\", \"r\") as transcript:\n",
        "    transcription = transcript.read()\n",
        "\n",
        "print(transcription)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhTKWKcB3DFw",
        "outputId": "590f01df-32c1-46ae-ee8b-47fd7de2488f"
      },
      "outputs": [
        {
          "ename": "TimeoutException",
          "evalue": "Requesting secret grok timed out. Secrets can only be fetched when running from the Colab UI.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cd59dffe3597>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mXAI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m llm = ChatAnthropic(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret grok timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import boto3\n",
        "from langchain_aws import ChatBedrock\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "# session = boto3.Session(\n",
        "#     aws_access_key_id=userdata.get('aws_access_key_id'),\n",
        "#     aws_secret_access_key=userdata.get('aws_secret_access_key'),\n",
        "#     region_name=\"ap-northeast-2\"\n",
        "# )\n",
        "\n",
        "# BEDROCK_CLIENT = session.client(\"bedrock-runtime\", 'ap-northeast-2')\n",
        "\n",
        "# llm = ChatBedrock(\n",
        "#     model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
        "#     model_kwargs=dict(temperature=0),\n",
        "#     client=BEDROCK_CLIENT,\n",
        "# )\n",
        "\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('groq')\n",
        "\n",
        "# llm = ChatGroq(\n",
        "#     model=\"llama-3.1-70b-versatile\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "# )\n",
        "\n",
        "# os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"nvidia\")\n",
        "\n",
        "# llm = ChatNVIDIA(\n",
        "#     model=\"nvidia/llama-3.1-nemotron-70b-instruct\"\n",
        "# )\n",
        "\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('anthropic')\n",
        "\n",
        "# llm = ChatAnthropic(\n",
        "#     model=\"claude-3-5-sonnet-20240620\",\n",
        "#     temperature=0,\n",
        "# )\n",
        "\n",
        "# transcription = response['results']['channels'][0]['alternatives'][0]['transcript']\n",
        "\n",
        "\n",
        "# template = \"\"\"\n",
        "# You are tasked with summarizing a YouTube video lecture. Your goal is to create a concise and informative summary that captures the main points and key takeaways from the lecture. Follow these instructions carefully to produce an effective summary.\n",
        "\n",
        "# Here is the transcript of the video lecture:\n",
        "\n",
        "# <transcript>\n",
        "# {TRANSCRIPT}\n",
        "# </transcript>\n",
        "\n",
        "# To summarize the lecture, please follow these steps:\n",
        "\n",
        "# 1. Carefully read through the entire transcript.\n",
        "# 2. Identify the main topic or theme of the lecture.\n",
        "# 3. Determine the key points, arguments, or concepts presented in the lecture.\n",
        "# 4. Note any important examples, case studies, or data that support the main ideas.\n",
        "# 5. Identify the overall structure or flow of the lecture.\n",
        "\n",
        "# Create your summary using the following format:\n",
        "\n",
        "# <summary>\n",
        "# Title: [Provide a concise title that captures the main topic of the lecture]\n",
        "\n",
        "# Main Topic: [Briefly state the overarching subject or theme of the lecture in 1-2 sentences]\n",
        "\n",
        "# Key Points:\n",
        "# 1. [First key point]\n",
        "# 2. [Second key point]\n",
        "# 3. [Third key point]\n",
        "#    [Add more points as needed, aiming for 3-5 in total]\n",
        "\n",
        "# Supporting Information:\n",
        "# - [Include 1-3 important examples, case studies, or pieces of data that reinforce the key points]\n",
        "\n",
        "# Conclusion: [Summarize the main takeaway or conclusion of the lecture in 1-2 sentences]\n",
        "# </summary>\n",
        "\n",
        "# Additional instructions:\n",
        "# - Keep your summary concise, aiming for about 200-300 words in total.\n",
        "# - Use clear and straightforward language.\n",
        "# - Focus on the most important information; avoid including minor details or tangential points.\n",
        "# - Ensure that your summary accurately reflects the content of the lecture without adding any external information or personal opinions.\n",
        "# - If the lecture contains technical terms or jargon, briefly explain them if they are crucial to understanding the main points.\n",
        "\n",
        "# Remember, your goal is to provide a clear and informative overview that would help someone quickly understand the main content of the lecture without watching the entire video.\n",
        "# \"\"\"\n",
        "\n",
        "template = \"\"\"\n",
        "I'd like to write a blog post with the insights from the provided transcript. list all important points with supporting quotes. Your answer must be in Korean.\n",
        "\n",
        "<transcript>\n",
        "{TRANSCRIPT}\n",
        "</transcript>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm | (lambda x: x.content)\n",
        "\n",
        "summary = chain.invoke({\"TRANSCRIPT\": transcription})\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9w9qV1kWPGo",
        "outputId": "29f3f0e5-afca-4514-c378-b442d1d38730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: 400 Client Error: Bad Request for url: https://api.notion.com/v1/pages\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "auth_token = userdata.get('notion')\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {auth_token}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Notion-Version\": \"2022-02-22\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "\t  \"parent\": { \"page_id\": \"11d11dd8536b801385c2d7d3faa6e8a1\" },\n",
        "    \"properties\": {\n",
        "        \"Name\": {\n",
        "            \"title\": [\n",
        "                {\n",
        "                    \"text\": {\n",
        "                        \"content\": \"Conversation with Groq CEO Jonathan Ross\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    \"children\": [\n",
        "\t\t{\n",
        "\t\t\t\"object\": \"block\",\n",
        "\t\t\t\"type\": \"heading_2\",\n",
        "\t\t\t\"heading_2\": {\n",
        "\t\t\t\t\"rich_text\": [{ \"type\": \"text\", \"text\": { \"content\": \"Lacinato kale\" } }]\n",
        "\t\t\t}\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"object\": \"block\",\n",
        "\t\t\t\"type\": \"paragraph\",\n",
        "\t\t\t\"paragraph\": {\n",
        "\t\t\t\t\"rich_text\": [\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\"type\": \"text\",\n",
        "\t\t\t\t\t\t\"text\": {\n",
        "\t\t\t\t\t\t\t\"content\": transcription,\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t]\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "\n",
        "# print(payload)\n",
        "\n",
        "try:\n",
        "    response = requests.post(\"https://api.notion.com/v1/pages\", headers=headers, data=payload)\n",
        "    response.raise_for_status()\n",
        "    print(response.json())\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KihZXtBXqxma"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODQAolZPbG2QKu1HSe60Ns",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}